[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Science",
    "section": "",
    "text": "Welcome\nThis is a set of notes developed for an undergraduate course in data science. The target audience for these notes are students who have completed a first course in either programming or data science.\n© Michael Linderman and Phil Chodrow, 2025",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#pedagogical-features",
    "href": "index.html#pedagogical-features",
    "title": "Applied Data Science",
    "section": "Pedagogical Features",
    "text": "Pedagogical Features\nThese notes are explicitly designed for undergraduate instruction. For this reason:\n\nComputational examples are integrated into the text and shown throughout.\nLive versions of lecture notes are supplied via Google Colab, with certain code components removed. The purpose is to facilitate live-coding in lectures.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#use-and-reuse",
    "href": "index.html#use-and-reuse",
    "title": "Applied Data Science",
    "section": "Use and Reuse",
    "text": "Use and Reuse\nThese notes were written by Michael Linderman and Phil Chodrow for the course CSCI 1010: Applied Data Science at Middlebury College. All are welcome to use them for educational purposes. Attribution is appreciated but not expected.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#source-texts",
    "href": "index.html#source-texts",
    "title": "Applied Data Science",
    "section": "Source Texts",
    "text": "Source Texts\nWe rely a lot on Vanderplas (2016).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Applied Data Science",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis site was generated using the Quarto publishing system. It is hosted on GitHub and published via GitHub Pages. Google Colab provides free access to interactive Python environments for live coding.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Applied Data Science",
    "section": "References",
    "text": "References\n\n\n\n\nVanderplas, Jacob T. 2016. Python Data Science Handbook: Essential Tools for Working with Data. First edition. Sebastopol, CA: O’Reilly Media, Inc.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/01-data-and-models.html",
    "href": "chapters/01-data-and-models.html",
    "title": "1  Data, Patterns, and Models",
    "section": "",
    "text": "Supervised Learning\nBroadly, machine learning is the science and practice of building algorithms that learn patterns from data. Here are two synthetic examples of data in which there is some kind visually clear pattern in the data. In the first example (Figure 1.1 (a)), we can see that there is a relationship between two quantitative variables, \\(x\\) and \\(y\\). If someone told is the value of \\(x\\), then we would likely be able to make a reasonable prediction about the value of the number \\(y\\). This is an example of a regression task. In the second example (Figure 1.1 (b)), we can observe a relation between the location of each data point in 2d space and a color, which could represent a category or class. If someone told us the location of the point in 2d space, we would likely be able to say which of the two classes it belonged to. This is an example of a classification task.\nIn examples like these, it’s often possible for us to just use our eyes and spatial reasoning in order to make a reasonable prediction.\nHow do we encode this knowledge mathematically, in a way that a computer can understand and use? In each of the two cases, we can express the pattern in the data as a mathematical function.\nOur focus in these notes is almost exclusively on supervised learning. In supervised learning, we are able to view some attributes or features of a data point, which we call predictors. Traditionally, we collect these attributes into a vector called \\(\\mathbf{x}\\). Each data point then has a target, which could be either a scalar number or a categorical label. Traditionally, the target is named \\(y\\). We aim to predict the target based on the predictors using a model, which is a function \\(f\\). The result of applying the model \\(f\\) to the predictors \\(\\mathbf{x}\\) is our prediction or predicted target \\(f(\\mathbf{x})\\), to which we often give the name \\(\\hat{y}\\). Our goal is to choose \\(f\\) such that the predicted target \\(\\hat{y}\\) is equal to, or at least close to, the true target \\(y\\). We could summarize this with the heuristic statement:\n\\[\n\\begin{aligned}\n    \\text{``}f(\\mathbf{x}) = \\hat{y} \\approx y\\;.\\text{''}\n\\end{aligned}\n\\]\nHow we interpret this heuristic statement depends on context. In regression problems, this statement typically means “\\(\\hat{y}\\) is usually close to \\(y\\)”, while in classification problems this statement usually means that “\\(\\hat{y} = y\\) exactly most or all of the time.”\nIn our regression example from above, we can think of a function \\(f:\\mathbb{R}\\rightarrow \\mathbb{R}\\) that maps the predictor \\(x\\) to the prediction \\(\\hat{y}\\). In the case of classification, things are a little more complicated. Although the function \\(g(x_1) = 1 - x_1\\) is visually very relevant, that function is not itself the model we use for prediction. Instead, our prediction function should return one classification label for points on one side of the line defined by that function, and a different label for points on the other side. If we say that blue points are labeled \\(0\\) and brown points are labeled \\(1\\), then our predictor function can be written \\(f:\\mathbb{R}^2 \\rightarrow \\{0, 1\\}\\), and it could be written heuristically like this:\n\\[\n\\begin{aligned}\n    f(\\mathbf{x}) &= \\mathbb{1}[\\mathbf{x} \\text{ is above the line}] \\\\\n                  &= \\mathbb{1}[x_1 + x_2 \\geq 1] \\\\\n                  &= \\mathbb{1}[x_1 + x_2 - 1\\geq 0]\\;.\n\\end{aligned}\n\\]\nThis last expression looks a little clunky, but we will soon find out that it is the easiest one to generalize to an advanced setting.\n© Michael Linderman and Phil Chodrow, 2025",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data, Patterns, and Models</span>"
    ]
  },
  {
    "objectID": "chapters/01-data-and-models.html#supervised-learning",
    "href": "chapters/01-data-and-models.html#supervised-learning",
    "title": "1  Data, Patterns, and Models",
    "section": "",
    "text": "Here, \\(\\mathbb{1}\\) is the indicator function which is equal to 1 if its argument is true and 0 otherwise. Formally,\n\\[\n\\begin{aligned}\n    \\mathbb{1}[P] = \\begin{cases}\n        1 &\\quad P \\text{ is true} \\\\\n        0 &\\quad P \\text{ is false.}\n        \\end{cases}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data, Patterns, and Models</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html",
    "href": "chapters/02-black-box-classification.html",
    "title": "2  Classification as a Black Box",
    "section": "",
    "text": "Classifying the Palmer Penguins\nIn these notes, we’ll make a lightning tour through the “standard workflow” for users of predictive machine learning technologies. Our focus will be on out-of-the-box Python tools for acquiring, visualizing, and analyzing tabular data sets.\nWe’re going to move pretty quickly through some big topics in practical data science: acquiring data, data visualization, data manipulation, and prediction using the Scikit-Learn package. Throughout these notes, I’ve sprinkled references to the Python Data Science Handbook (Vanderplas 2016), which treats many of these practical considerations in much greater detail.\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins.\nLet’s go ahead and acquire the data.\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nnp.set_printoptions(precision = 3)\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\nLet’s take a look:\ndf.head() # first 5 rows\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\nIt’s always useful to get acquainted with the “basics” of the data. For example, how many rows and columns do we have?\ndf.shape # (rows, columns)\n\n(344, 17)\ndf.dtypes \n\nstudyName               object\nSample Number            int64\nSpecies                 object\nRegion                  object\nIsland                  object\nStage                   object\nIndividual ID           object\nClutch Completion       object\nDate Egg                object\nCulmen Length (mm)     float64\nCulmen Depth (mm)      float64\nFlipper Length (mm)    float64\nBody Mass (g)          float64\nSex                     object\nDelta 15 N (o/oo)      float64\nDelta 13 C (o/oo)      float64\nComments                object\ndtype: object\nHere’s the question we’ll ask today about this data set:\n© Michael Linderman and Phil Chodrow, 2025",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#classifying-the-palmer-penguins",
    "href": "chapters/02-black-box-classification.html#classifying-the-palmer-penguins",
    "title": "2  Classification as a Black Box",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\n\nThe Palmer Penguins data was originally collected by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020). You can find a very concise summary of the main workflow using a similar data set in Vanderplas (2016).\n\n\n\n\n The df variable holds a pandas.DataFrame object. You can think of a data frame as a table of data with a variety of useful behaviors for data manipulation and visualization.You can learn much more about the capabilities of pandas.DataFrame objects in Chapter 3 of Vanderplas (2016)\n\n\n\n\nWhat are the data types of the columns? str columns are represented with the generic object in Pandas.\n\n\n\nGiven some physiological measurements of a penguin, can we reliably infer its species?",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#data-preparation",
    "href": "chapters/02-black-box-classification.html#data-preparation",
    "title": "2  Classification as a Black Box",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe can select our desired columns from the data frame, operate on them, and make assignments to them using the data-frame-as-dictionary paradigm explored in Vanderplas (2016).\nIn applied data science, at least 80% of the work is typically spent acquiring and preparing data. Here, we’re going to do some simple data preparation directed by our question. It’s going to be convenient to shorten the Species column for each penguin. Furthermore, for visualization purposes today we are going to focus on the Culmen Length (mm) and Culmen Depth (mm) columns.\n\n# use only these three columns\ndf = df[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Species\"]] \n\n# remove any rows that have missing data in any of the selected columns. \ndf = df.dropna()\n\n# slightly advanced syntax: \n# replace the column with the first word in each entry\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\nLet’s take a look at what we’ve done so far:\n\ndf.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nSpecies\n\n\n\n\n0\n39.1\n18.7\nAdelie\n\n\n1\n39.5\n17.4\nAdelie\n\n\n2\n40.3\n18.0\nAdelie\n\n\n4\n36.7\n19.3\nAdelie\n\n\n5\n39.3\n20.6\nAdelie\n\n\n\n\n\n\n\nAs another preprocessing step, we are going to add transformed labels represented as integers.\n\n# for later: assign an integer to each species\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf[\"species_label\"] = le.fit_transform(df[\"Species\"])\n\nfor i, c in enumerate(le.classes_):\n    print(f\"Class number {i} represents {c} penguins.\")\n\nClass number 0 represents Adelie penguins.\nClass number 1 represents Chinstrap penguins.\nClass number 2 represents Gentoo penguins.\n\n\nNow our data looks like this:\n\ndf.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nSpecies\nspecies_label\n\n\n\n\n0\n39.1\n18.7\nAdelie\n0\n\n\n1\n39.5\n17.4\nAdelie\n0\n\n\n2\n40.3\n18.0\nAdelie\n0\n\n\n4\n36.7\n19.3\nAdelie\n0\n\n\n5\n39.3\n20.6\nAdelie\n0\n\n\n\n\n\n\n\n\nTrain-Test Split\nWhen designing predictive models, it’s important to evaluate them in a context that simulates the prediction application as accurately as possible. One important way we do this is by performing a train-test split. We keep most of the data as training data which we’ll use to design the model. We’ll hold out a bit of the data as testing data, which we’ll treat as unseen and only use once we are ready to evaluate our final design. The testing data simulates the idea of “new, unseen data” – exactly the kind of data on which it would be useful for us to make predictions!\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size = 0.2)\n\nLet’s check the size of our two split data sets:\n\ndf_train.shape, df_test.shape\n\n((273, 4), (69, 4))\n\n\nNow we’re going to forget that df_test exists for a while. Instead, we’ll turn our attention to analysis, visualization and modeling.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#data-analysis-and-visualization",
    "href": "chapters/02-black-box-classification.html#data-analysis-and-visualization",
    "title": "2  Classification as a Black Box",
    "section": "Data Analysis and Visualization",
    "text": "Data Analysis and Visualization\nAs a first step, it’s useful to understand how many of each species there are in the training data:\nThis is an example of a “split-apply-combine” operation (Wickham 2011). We split the dataframe into three groups depending on the species label, apply an operation (in this case, computing the number of rows), and then combine the results into a single object. Pandas implements split-apply-combine primarily through the groupby method and several associated functions. There are some nice examples of split-apply-combine in Pandas in Vanderplas (2016).\n\ndf_train.groupby(\"Species\").size()\n\nSpecies\nAdelie       115\nChinstrap     53\nGentoo       105\ndtype: int64\n\n\nThere are more Adelie penguins than Chintraps or Gentoos in this data set. Here are the proportions:\n\ndf_train.groupby(\"Species\").size() / df_train.shape[0] # divide by total rows\n\nSpecies\nAdelie       0.421245\nChinstrap    0.194139\nGentoo       0.384615\ndtype: float64\n\n\nSo, over 40% of the penguins in the data are Adelie penguins. One important consequence of this proportion is the base rate of the classification problem. The base rate refers to how well we could perform at prediction if we did not use any kind of predictive modeling, but instead simply predicted the most common class for every penguin. Here, if we always predicted “Adelie” for the species, we’d expect to be right more than 40% of the time. So, a minimal expectation of anything fancier we do is that it should be correct much more than 40% of the time.\nNow let’s take a look at our (training) data and see whether our chosen columns look like they have a chance of predicting the penguin species. We’ll show the plot both without and with the species labels.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(df_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(df_train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\", style = \"Species\", ax = ax[1])\n\n\n\n\nThese plots are generated using the Seaborn library for Python. Seaborn is a high-level wrapper around the classical matplotlib library for data visualization. Although Matplotlib is very flexible, Seaborn is optimized for visualizing data contained in Pandas data frames. You can find many examples of creating Seaborn plots in the official gallery, and many tips and examples for matplotlib in Vanderplas (2016).\n\n\n\n\nWe can think of the lefthand side as “what the model will see:” just physiological measurements with no labels. On the right we can see the data with its species labels included. We can see that the species are divided into clusters: Adelie penguins have measurements which tend to be similar to other Adelies; Chinstraps are similar to other Chinstraps, etc.\nThis pattern is promising! The approximate separation of the species suggests that a machine learning model which predicts the species label from these measurements is likely to be able to beat the base rate.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#modeling-and-model-selection",
    "href": "chapters/02-black-box-classification.html#modeling-and-model-selection",
    "title": "2  Classification as a Black Box",
    "section": "Modeling and Model Selection",
    "text": "Modeling and Model Selection\nLet’s go ahead and fit some models! We’re going to fit two models that are pre-implemented in the package scikit-learn. For now, you can think of these models as black-box algorithms that accept predictor variables as inputs and return a predicted target as an output. In our case, the predictor variables are the culmen length and culmen depth columns, while the target we are attempting to predict is the species. Later on, we’ll learn more about how some of these models actually work.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\nIt’s convenient to split our data into predictors \\(\\mathbf{X}\\) and targets \\(\\mathbf{y}\\). We need to do this once for each of the training and test sets.\n\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\ntarget_col = \"species_label\"\n\nX_train = df_train[predictor_cols]\ny_train = df_train[target_col]\n\nX_test = df_test[predictor_cols]\ny_test = df_test[target_col]\n\nLet’s take a quick look at X_train\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n309\n52.1\n17.0\n\n\n136\n35.6\n17.5\n\n\n260\n42.7\n13.7\n\n\n279\n50.4\n15.3\n\n\n14\n34.6\n21.1\n\n\n...\n...\n...\n\n\n98\n33.1\n16.1\n\n\n148\n36.0\n17.8\n\n\n238\n46.2\n14.5\n\n\n220\n46.1\n13.2\n\n\n214\n45.7\n17.0\n\n\n\n\n273 rows × 2 columns\n\n\n\nWe’ll go in-depth on logistic regression later in this course.\nNow we’re ready to fit our first machine learning model. Let’s try logistic regression! In the Scikit-learn API, we first need to instantiate the LogisticRegression() class, and then call the fit() method of this class on the training predictors and targets.\n\nLR = LogisticRegression()\nm = LR.fit(X_train, y_train)\n\nSo, uh, did it work? The LogisticRegression() class includes a handy method to compute the accuracy of the classifier:\n\nLR.score(X_train, y_train)\n\n0.9633699633699634\n\n\nWow! Much better than the base rate. Note that this is the accuracy on the training data. In theory, accuracy on the test data could look very different.\nA useful way to visualize models with two numerical predictors is via decision regions. Each region describes the set of possible measurements that would result in a given classification.\nYou can unfold this code to see a simple implementation of a function for plotting decision regions which wraps the plot_decision_regions function of the mlxtend package.\n\n\nCode\ndef decision_regions(X, y, model, title):\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        ax = plot_decision_regions(X_train.to_numpy(), y_train.to_numpy(), clf = model, legend = 2)\n\n        handles, labels = ax.get_legend_handles_labels()\n        ax.legend(handles, \n                le.classes_, \n                framealpha=0.3, scatterpoints=1)\n\n        ax.set(xlabel = \"Culmen Length (mm)\", ylabel = \"Culmen Depth (mm)\", title = f\"{title}: Accuracy = {model.score(X, y):.3f}\")\n\ndecision_regions(X_train, y_train, LR, \"Decision Regions for Logistic Regression\")\n\n\n\n\n\n\n\n\n\nYou can learn more about how support vector machines work in Vanderplas (2016). We’ll also study these models later in the course.\nWhile we’re at it, let’s try fitting a different classifier, also supplied by Scikit-learn. This classifier is called support vector machine (SVM).\n\nSVM = SVC(gamma = 5)\nSVM.fit(X_train, y_train)\ndecision_regions(X_train, y_train, SVM, \"Decision Regions for Support Vector Machine\")\n\n\n\n\n\n\n\n\nWow! The support vector machine classifier achieved even higher accuracy on the training data. This is enabled by the greater flexibility of the SVM. Flexibility comes from a lot of places in machine learning, and generally refers to the ability of models to learn complicated decision boundaries like the ones shown here.\nBut is this increased flexibility a good thing? You might look at this predictor and think that something funny is going on. For example, shouldn’t a point on the bottom right be more likely to be a Gentoo penguin than an Adelie?…\n\nSimulating Evaluation: Cross-Validation\nNow we have two competing classification models: logistic regression and support vector machine. Which one is going to do the best job of prediction on totally new, unseen data? We could go ahead and evaluate on our test set, but for statistical reasons we need to avoid doing this until we’ve made a final choice of classifier.\nVanderplas (2016) has more on cross-validation and overfitting. We’ll confront overfitting agian many times in this course.\nIn order to make an assessment, we can simulate the process of fitting the model and evaluating on “test” data by witholding parts of our training data to use as testing. We split the data into chunks and withold each chunk, using the other chunks to train the data. This is called cross-validation, and it is illustrated in this figure:\n\n\n\nImage source: scikit-learn\n\n\nWe could do this with a janky for-loop, but the nice scikit-learn developers have implemented this for us. Here’s an example of cross-validation with 5 folds. This can take a little while, as there are actually 5 calls to model.fit() happening under the hood each time.\n\nfrom sklearn.model_selection import cross_val_score\n\nFirst let’s compute the cross-validation accuracies for logistic regression:\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\ncv_scores_LR\n\narray([0.982, 0.982, 0.891, 0.944, 0.963])\n\n\nA convenient way to summarize these results is by computing the average:\n\ncv_scores_LR.mean()\n\nnp.float64(0.9523905723905723)\n\n\nLet’s compare to SVM:\n\ncv_scores_SVM = cross_val_score(SVM, X_train, y_train, cv=5)\ncv_scores_SVM.mean()\n\nnp.float64(0.8682154882154883)\n\n\nAh! It looks like our SVM classifier was indeed too flexible to do well in predicting data that it hasn’t seen before. Although the SVM had better training accuracy than the logistic regression model, it failed to generalize to the task of unseen prediction. This phenomenon is called overfitting. Dealing with overfitting is one of the fundamental modeling challenges in applied machine learning.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#model-evaluation",
    "href": "chapters/02-black-box-classification.html#model-evaluation",
    "title": "2  Classification as a Black Box",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nSo far, we’ve fit a logistic regression model and a support vector machine model; compared the two on a cross-validation task; and determined that the logistic regression model is most likely to generalize. Let’s now retrain the logistic regression model on the complete training data and finally evaluate it on the test set:\n\nLR.fit(X_train,y_train) \nLR.score(X_test, y_test)\n\n0.9420289855072463\n\n\nNot bad! This is our final estimate for the accuracy of our model as a classification tool on unseen penguin data.\n\nBeyond Accuracy\nAccuracy is a simple measure of how many errors a model makes. In many applications, it’s important to understand what kind of errors the model makes, a topic which we’ll study much more when we come to decision theory in the near future. We can get a quick overview of the kinds of mistakes that a model makes by computing the confusion matrix between the true labels and predictions. This matrix cross-tabulates all the true labels with all the predicted ones.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[36,  0,  0],\n       [ 1, 11,  3],\n       [ 0,  0, 18]])\n\n\nThe entry in the ith row and jth column of the confusion matrix gives the number of data points that have true label i and predicted label j from our model.\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 36 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 1 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 3 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 18 Gentoo penguin(s) who were classified as Gentoo.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#recap",
    "href": "chapters/02-black-box-classification.html#recap",
    "title": "2  Classification as a Black Box",
    "section": "Recap",
    "text": "Recap\nIn these notes, we took a very quick tour of the core data science workflow. We considered a simple classification problem in which we acquired some data, cleaned it up a bit, visualized several of its features, used those features to make a predictive classification model, visualized that model, and evaluated its accuracy. Along the way, we encountered the phenomenon of overfitting: models that are too flexible will achieve remarkable accuracy on the training set but will generalize poorly to unseen data. The problem of designing models that are “flexible enough” and “in the right way” is a fundamental driving force in modern machine learning, and the deep learning revolution can be viewed as the latest paradigm for seeking appropriately flexible models.\nSo far, we haven’t attempted to understand how any of these predictive models actually work. We’ll dive into this topic soon.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/02-black-box-classification.html#references",
    "href": "chapters/02-black-box-classification.html#references",
    "title": "2  Classification as a Black Box",
    "section": "References",
    "text": "References\n\n\n\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” Edited by André Chiaradia. PLoS ONE 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison M, Alison Presmanes Hill, and Kristen B Gorman. 2020. “Allisonhorst/Palmerpenguins: V0.1.0.” Zenodo. https://doi.org/10.5281/ZENODO.3960218.\n\n\nVanderplas, Jacob T. 2016. Python Data Science Handbook: Essential Tools for Working with Data. First edition. Sebastopol, CA: O’Reilly Media, Inc.\n\n\nWickham, Hadley. 2011. “The Split-Apply-Combine Strategy for Data Analysis.” Journal of Statistical Software 40 (1). https://doi.org/10.18637/jss.v040.i01.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Classification as a Black Box</span>"
    ]
  },
  {
    "objectID": "chapters/20-introduction-prediction.html",
    "href": "chapters/20-introduction-prediction.html",
    "title": "3  Introducing Prediction",
    "section": "",
    "text": "Linear Regression\nIn the task of automated prediction, we aim to use a relationship between a set of input features (also called predictors or independent variables) and an output (also called a target). We use this relationship to make a prediction about what outputs would be observed from new inputs.\nFor example, suppose we have a numerical input \\(x\\) and a numerical output \\(y\\), and suppose we knew that the input \\(x\\) and the output \\(y\\) were related according to the following linear relationship: \\[\n\\begin{aligned}\ny = f(x) = 2x + 1\n\\end{aligned}\n\\]\nLet’s visualize this relationship:\nIf we knew this input-output relationship, then we could use it to make predictions for any value of \\(x\\). For example, if we observed an input value of \\(x = 0.5\\), we could predict that the corresponding output would be \\(y = 2(0.5) + 1 = 2\\):\nThe problem, however, is that nature very rarely gives us a true mathematical relationship between inputs and outputs. Instead, we get data. Rather than knowing the true relationship \\(y = f(x)\\), we instead observe a set of input-output pairs \\((x_i, y_i)\\) for \\(i = 1, \\ldots, n\\). What we are given looks much more like this:\nOur goal is to use this data to learn an approximate relationship \\(\\hat{y} = \\hat{f}(x)\\) that we can then use to make predictions for new input values.\nTechnically, the problem of learning a relationship between numerical inputs and a numerical output is called regression. We’ll consider linear regression, which assumes that the relationship between input and output is linear. In the case of a single input \\(x\\) and output \\(y\\), this means that we are trying to learn a relationship of the form:\n\\[\n\\begin{aligned}\n    y \\approx \\hat{y} = \\hat{f}(x) = wx + b\n\\end{aligned}\n\\]\nHere, \\(\\hat{y}\\) is the name that we’ll give to our model’s predicted output, which is hopefully close to the true output \\(y\\). The predicted output is a linear function of the input \\(x\\). In this function, \\(w\\) is the slope of the line and \\(b\\) is the intercept. We call \\(w\\) and \\(b\\) the model parameters. The task of fitting the model is the task of finding values of \\(w\\) and \\(b\\) that make the model’s predictions \\(\\hat{y}\\) as close as possible to the observed outputs \\(y\\) in the training data.\nLet’s construct a data frame with several candidate examples of model parameters \\(w\\) and \\(b\\):\nk_params = 5\n\nmodel_df = pd.DataFrame({\n    'w': 2*(np.random.rand(k_params) - 0.5) + 2,\n    'b': 2*(np.random.rand(k_params) - 0.5) + 1\n})\n\nmodel_df\n\n\n\n\n\n\n\n\nw\nb\n\n\n\n\n0\n2.624528\n1.089407\n\n\n1\n1.229894\n1.434793\n\n\n2\n2.739733\n0.645119\n\n\n3\n1.513494\n1.360442\n\n\n4\n1.601168\n0.600460\nEach row of this data frame corresponds to a different candidate linear model. Let’s visualize their predictions:\nfor i in range(k_params):     \n    w = model_df.loc[i, 'w']\n    b = model_df.loc[i, 'b']\n    \n    df[\"y_pred\"] = w * df[\"x\"] + b\n    sns.lineplot(data=df, x='x', y='y_pred', ax=ax, label = f'Model {i}')\n\nfig_1\n\n\n\n\n\n\n\nFigure 3.4: Predictions from several candidate linear regression models.\nEach of the models gives different predictions \\(\\hat{y}\\) given the same input \\(x\\). How do we determine which of these models is “best”? A reasonable intuition here is that a good model should make predictions \\(\\hat{y}\\) which are close to the obserevd outputs \\(y\\) on average. One way to express the idea of closeness is to consider the residuals \\(r_i = y_i - \\hat{y}_i\\), which measure the difference between the observed output \\(y_i\\) and the predicted output \\(\\hat{y}_i\\) for each data point \\(i\\):\nfig, ax = plt.subplots(figsize=(6, 4))\n\nsns.scatterplot(data=df, x='x', y='y', ax=ax, edgecolors = \"black\", facecolors='white', s=50)\n\n# example model parameters\nw = 2.1\nb = 0.9\n\ndf['y_pred'] = w * df['x'] + b\n\n# plot regression line\nx_vals = np.linspace(df['x'].min(), df['x'].max(), 200)\nax.plot(x_vals, w * x_vals + b, \n        color='black', \n        linewidth=2, \n        label=fr'Model: $\\hat{{y}}={w:.1f}x+{b:.1f}$')\n\n# draw residuals as vertical lines from observed y to predicted y\nax.vlines(df['x'], \n          df['y'], \n          df['y_pred'], \n          color='gray', \n          linestyle='--', \n          alpha=0.8, \n          linewidth=1, \n          label = \"Residuals\", \n          zorder = -10)\n\nax.legend()\n\n\n\n\n\n\n\nFigure 3.5: Residuals for a candidate linear regression model.\nLet’s quantify the idea of closeness using these residuals. The following function accepts three arguments: the model_df containing coefficients, the data df containing x and y, and a metric function that quantifies the closeness between y and predicted y_pred. The result is a column of scores computed according to metric.\ndef score_model(model_df, df, metric):\n    return model_df.apply(lambda row: metric(df[\"y\"], row['w'] * df[\"x\"] + row['b']), axis=1)\nFor example, suppose we use the mean residuals to score the candidate models from above. In this case, the model’s score, which we’ll call \\(L\\), would be:\n\\[\n\\begin{aligned}\n    L = \\frac{1}{n} \\sum_{i=1}^{n} r_i = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )\n\\end{aligned}\n\\]\nLet’s try it out:\ndef mean_residuals(y, y_pred):\n    return (y_pred - y).mean()\n\nmodel_df[\"mean_residuals\"] = score_model(model_df, df, mean_residuals)\nmodel_df.sort_values(\"mean_residuals\")\n\n\n\n\n\n\n\n\nw\nb\nmean_residuals\n\n\n\n\n4\n1.601168\n0.600460\n-0.592038\n\n\n1\n1.229894\n1.434793\n0.040985\n\n\n2\n2.739733\n0.645119\n0.069967\n\n\n3\n1.513494\n1.360442\n0.120406\n\n\n0\n2.624528\n1.089407\n0.451790\nIf we compare the mean residuals for each model to Figure 3.4, we can see that our use of the mean residual as a scoring metric tends to prefer models that under-predict (i.e. models with negative mean residuals). That doesn’t seem to be a great idea, and doesn’t match our intuition for which of the models above best “fits” the data.\nA more standard idea is to use not the mean error, but the mean squared error (MSE) as a scoring metric. The MSE is defined as:\n\\[\n\\begin{aligned}\n    \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2\n\\end{aligned}\n\\]\nWe can implement this metric in Python as follows:\ndef mse(y, y_pred):\n    return ((y_pred - y) ** 2).mean()\nNow let’s use the MSE to score our candidate models:\nmodel_df[\"mse\"] = score_model(model_df, df, mse)\nmodel_df.sort_values(\"mse\", inplace=True)\nmodel_df\n\n\n\n\n\n\n\n\nw\nb\nmean_residuals\nmse\n\n\n\n\n3\n1.513494\n1.360442\n0.120406\n0.044986\n\n\n1\n1.229894\n1.434793\n0.040985\n0.048375\n\n\n2\n2.739733\n0.645119\n0.069967\n0.060129\n\n\n0\n2.624528\n1.089407\n0.451790\n0.250468\n\n\n4\n1.601168\n0.600460\n-0.592038\n0.377655\nThe mse identifies model np.int64(3) as the best model among our candidates. Comparing to Figure 3.4 suggests that the preferred model under the MSE does indeed seem to be one that fits the data better.\n© Michael Linderman and Phil Chodrow, 2025",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/20-introduction-prediction.html#working-with-real-data-abalone-age-prediction",
    "href": "chapters/20-introduction-prediction.html#working-with-real-data-abalone-age-prediction",
    "title": "3  Introducing Prediction",
    "section": "Working With Real Data: Abalone Age Prediction",
    "text": "Working With Real Data: Abalone Age Prediction\nLet’s now try linear regression with some real data. Abalone are a type of sea snail which are often harvested for food and jewelry.\n\n\n\n\n\nSeveral red abalone, from a vendor selling them for food. Image credit: Hog Island Oyster Co. \n\n\nDetermining the age of an abalone is an important task for sustainably managing the population. The age of an abalone can be determined by counting the number of rings on its shell, but this is very time-consuming relative to measuring other physical characteristics of the abalone like its size and weight. It would be helpful if we could reliably predict the age (number of rings) of an abalone from these other physical characteristics. Let’s download a data set to help us do this:\n\nurl = \"https://raw.githubusercontent.com/middcs/data-science-notes/refs/heads/main/data/abalone/abalone.csv\"\ndf = pd.read_csv(url)\n\nThe abalone data set (Nash and Ford 1994) contains measurements for 4177 individual abalones. Here’s how the data looks: We have unscaled the numerical columns back to their original units.\n\ndf.head()\n\n\n\n\n\n\n\n\nsex\nlength (mm)\ndiameter (mm)\nheight (mm)\nwhole_weight (g)\nshucked_weight (g)\nviscera_weight (g)\nshell_weight (g)\nrings\n\n\n\n\n0\nM\n91.0\n73.0\n19.0\n102.8\n44.9\n20.2\n30.0\n15\n\n\n1\nM\n70.0\n53.0\n18.0\n45.1\n19.9\n9.7\n14.0\n7\n\n\n2\nF\n106.0\n84.0\n27.0\n135.4\n51.3\n28.3\n42.0\n9\n\n\n3\nM\n88.0\n73.0\n25.0\n103.2\n43.1\n22.8\n31.0\n10\n\n\n4\nI\n66.0\n51.0\n16.0\n41.0\n17.9\n7.9\n11.0\n7\n\n\n\n\n\n\n\nThe sex column is qualitative (Male, Female, Infant), while the other columns give numerical units. The length, diameter, and height columns are in millimeters, while the whole_weight, shucked_weight, viscera_weight, and shell_weight columns are in grams. The rings column gives the number of rings on the abalone’s shell, which is our target variable. If we take a look at a few of the numerical columns, we can see that they tend to correlate with rings, giving us hope that we can use them as predictors:\n\nsns.pairplot(df, \n             x_vars = [\"height (mm)\", \"length (mm)\", \"shell_weight (g)\"], \n             y_vars = [\"rings\"], \n             plot_kws={'alpha':0.5}, )\n\n\n\n\n\n\n\nFigure 3.6: Pairplot showing relationships between several physical characteristics of abalones and their age (number of rings).\n\n\n\n\n\nThe column whole_weight is a composite of the shucked_weight (the body of the abalone), viscera_weight (the gut), and shell_weight (the shell, after drying). The whole_weight is not exactly equal to the sum of these three weights, but it is very strongly correlated with them:\n\ndf[\"summed_weight (g)\"]  = df[\"shucked_weight (g)\"] + df[\"viscera_weight (g)\"] + df[\"shell_weight (g)\"]\nsns.scatterplot(data=df, \n                x='whole_weight (g)', \n                y='summed_weight (g)', \n                alpha = 0.2)\n\n\n\n\n\n\n\nFigure 3.7: Strong correlation (linear trend) between whole weight and the sum of shucked, viscera, and shell weights.\n\n\n\n\n\nLet’s therefore drop the whole_weight column to avoid redundancy:\n\ndf.drop(columns=[\"whole_weight (g)\"], axis=1, inplace=True)\n\nNow we can try building linear regression models to predict rings in terms of the other features. Here’s an example of doing this by hand: we’ll try out the model\n\\[\n\\begin{aligned}\n    \\text{rings} \\approx  15 \\times \\text{diameter} + 4 \\times \\text{shucked weight} + 1.5\n\\end{aligned}\n\\]\n\nw_1_guess = 15\nw_2_guess = 4\nb_guess = 1.5\n\ndf['rings_pred'] = w_1_guess * df['diameter (mm)'] + w_2_guess * df['shucked_weight (g)'] + b_guess\n\nmse_value = mse(df['rings'], df['rings_pred'])\n\nThis model has an MSE of mse_value=2.5e+06. Now that we’re done with the rings_pred column, let’s drop it and the summed_weight column we created earlier:\n\ndf.drop(columns=['rings_pred', 'summed_weight (g)'], inplace=True)\n\nWe could imagine trying to loop through different values of the parameters to find a better model, but this would be cumbersome, especially if we wanted to use more than two features. To address this, we’ll introduce the idea of model training as an optimization problem, and take a look at the scikit-learn library for automating this process.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/20-introduction-prediction.html#sidebar-training-as-optimization",
    "href": "chapters/20-introduction-prediction.html#sidebar-training-as-optimization",
    "title": "3  Introducing Prediction",
    "section": "Sidebar: Training as Optimization",
    "text": "Sidebar: Training as Optimization\nA general version of a linear regression model with many features can be written as follows:\n\\[\n\\begin{aligned}\n    \\hat{y} = w_1 x_1 + w_2 x_2 + \\ldots + w_p x_p + b = \\sum_{j=1}^{p} w_j x_j + b\n\\end{aligned}\n\\]\nThe MSE for this model is given by\n\\[\n\\begin{aligned}\n    \\text{MSE}(w_1, w_2, \\ldots, w_p, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\left( \\sum_{j=1}^{p} w_j x_{ij} + b \\right) \\right)^2\\;.\n\\end{aligned}\n\\]\nTo train the model, we’ll choose parameters \\(w_1, w_2, \\ldots, w_p, b\\) to minimize the MSE; that is, we’ll find these parameters to make the MSE as small as possible. Formally, we often write this as\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{aligned}\n    \\hat{w}_1, \\hat{w}_2,\\ldots, \\hat{w}_p, \\hat{b} = \\argmin_{w_1, w_2, \\ldots, w_p, b} \\text{MSE}(w_1, w_2, \\ldots, w_p, b)\\;.\n\\end{aligned}\n\\]\nThis is an optimization problem. The challenge of finding an optimal value for the parameters is a very rich one with many interesting solution algorithms. For our purposes in this class, we’ll take advantage of software packages which have already implemented these algorithms for us.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/20-introduction-prediction.html#scikit-learn-for-linear-regression",
    "href": "chapters/20-introduction-prediction.html#scikit-learn-for-linear-regression",
    "title": "3  Introducing Prediction",
    "section": "scikit-learn for Linear Regression",
    "text": "scikit-learn for Linear Regression\nThe scikit-learn package in Python, also called sklearn, is one such package.\n\nfrom sklearn.linear_model import LinearRegression\n\nWe’ll use this model to fit a linear regression model to predict rings from all the other features in the abalone data set. First, we need to convert the qualitative sex column into numerical columns using one-hot encoding:\n\ndf = pd.get_dummies(df, columns=['sex'], drop_first=True, dtype=int)\n\ndf.columns\n\nIndex(['length (mm)', 'diameter (mm)', 'height (mm)', 'shucked_weight (g)',\n       'viscera_weight (g)', 'shell_weight (g)', 'rings', 'sex_I', 'sex_M'],\n      dtype='object')\n\n\nNow instead of a single column for sex, we have two binary columns: sex_I and sex_M, indicating with a 1 whether the abalone is an infant or male. If both colummns are zero, then the abalone is female instead.\nThe scikit-learn API expects that we pass in a data frame X containing the features (inputs) and a series y containing the target (output). Let’s create these now:\n\nX = df.drop(columns=['rings'])\ny = df['rings']\n\nNow, finally, we are ready to fit the linear regression model. Finding the model that has the best values of the parameters is as simple as calling the fit method on a LinearRegression object:\n\nLR = LinearRegression()\nfit = LR.fit(X, y)\n\nWe can extract predictions for the model using the predict method:\n\ny_pred = LR.predict(X)\n\nOnce we have the predictions, we can also extract the MSE for the model:\n\nmse(y, y_pred)\n\nnp.float64(4.979110080607066)\n\n\nThis MSE is considerably smaller than the MSE we made with our by-hand model earlier!\nIt can also be helpful to look at the coefficients that the model learned for each feature:\n\npd.DataFrame({\n    'feature': LR.feature_names_in_,\n    'coefficient': LR.coef_\n}).sort_values(by='coefficient', ascending=False)\n\n\n\n\n\n\n\n\nfeature\ncoefficient\n\n\n\n\n5\nshell_weight (g)\n0.099027\n\n\n1\ndiameter (mm)\n0.058149\n\n\n2\nheight (mm)\n0.055457\n\n\n7\nsex_M\n0.051251\n\n\n0\nlength (mm)\n-0.003549\n\n\n4\nviscera_weight (g)\n-0.003759\n\n\n3\nshucked_weight (g)\n-0.057041\n\n\n6\nsex_I\n-0.872555\n\n\n\n\n\n\n\nThese coefficients can be quite helpful, although the interpretation is often subtle. It’s tempting to believe that larger coefficients mean that a given feature is “more important” for predicting the target, but things are not usually this simple. For example, notice that two of the weight features actually have negative coefficients, suggesting, for example, that a larger value of shucked_weight (g) should correspond to a younger abalone! This comes from the fact that shucked_weight is strongly correlated with shell_weight (g) and viscera_weight (g). This phenomenon, called multicollinearity in the statistics literature, can make interpreting coefficients quite tricky. We’ll revisit this issue later in the course. In the context of prediction, it’s often not necessary to interpret these coefficients at all.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/20-introduction-prediction.html#references",
    "href": "chapters/20-introduction-prediction.html#references",
    "title": "3  Introducing Prediction",
    "section": "References",
    "text": "References\n\n\n\n\nNash, Sellers, Warwick, and Wes Ford. 1994. “Abalone.” UCI Machine Learning Repository.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/21-feature-engineering-and-overfitting.html",
    "href": "chapters/21-feature-engineering-and-overfitting.html",
    "title": "4  Features and Overfitting",
    "section": "",
    "text": "Nonlinear Prediction: Feature Maps\nLast time, we began our study of predictive modeling. We introduced the idea of fitting a model as an optimization problem, in which our aim is to minimize some loss function which measures the error between model predictions and the actual data. We then used this minimization idea to fit a linear regression model to a data set and measure how the fit model reduces the error when compared to a naive baseline model.\nHowever, in most of the systems we might wish to study, the relationship between the features (input) and target (output) is nonlinear. In this set of notes, we’ll consider the problem of modeling nonlinear trends in data. Our approach to this problem will involve engineering many new features, which will in turn lead us to one of the fundamental risks in predictive modeling: overfitting.\nTo begin, let’s consider some synthetic data. We’ll write a simple function to generate data from a noisy sine wave:\nnp.random.seed(42)\n\ndef sin_data(n_samples=50, noise_std=0.3):\n    x = np.linspace(-3, 3, n_samples)\n    y = np.sin(x) + np.random.normal(0, noise_std, n_samples)\n    return pd.DataFrame({'x': x, 'y': y})\nThe resulting data set looks like this:\ndf = sin_data()\nax = sns.scatterplot(data=df, x='x', y='y')\nIf we try to just fit a linear regression model, we’ll be disappointed:\ndef mse(y, y_pred): \n    return np.mean((y - y_pred)**2)\n\nX = df[['x']]\ny = df['y']\nmodel = LinearRegression()\nmodel.fit(X, y)\ny_pred = model.predict(X)\n\nax = sns.scatterplot(data=df, x='x', y='y')\nsns.lineplot(x=df['x'], y=y_pred, color='black', ax =ax)\n\nt = ax.set_title(\n    f'Linear Regression MSE on training data: {mse(y, y_pred):.3f}'\n    )\nOne common paradigm for modeling nonlinear relationships is to engineer new features from the original data. This can be done by applying any nonlinear transformation to the original features. For example, we might define a set of feature maps \\(\\phi_d:\\mathbb{R} \\rightarrow \\mathbb{R}\\) which accept an input \\(x\\) and raise it to a power: \\[\n\\begin{aligned}\n    \\phi_d(x) = x^d\\;.\n\\end{aligned}\n\\]\nWe can define as many of these as we like! Once we’ve computed these features, we can form a linear regression model using the expanded feature set:\n\\[\n\\begin{aligned}\n    \\hat{y} = w_D\\phi_D(x) + w_{D-1}\\phi_{D-1}(x) + \\cdots + w_1 \\phi_1(x) + w_0 \\phi_0(x) = \\sum_{d=0}^D w_d \\phi_d(x)\\;.\n\\end{aligned}\n\\]\nNote that \\(\\phi_0(x) = x^0 = 1\\), so here \\(w_0\\) is playing the role of the bias term which we previously labeled \\(b\\). This combination of linear regression with a polynomial feature map is often called polynomial regression.\nscikit-learn gives us a convenient way to implement polynomial regression with two components: the PolynomialFeatures preprocessor, which computes the polynomial features, and the Pipeline class, which allows us to chain together multiple steps (like preprocessing and estimation) into a single model.\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef polynomial_regression(deg):\n    return Pipeline(steps=[\n        ('preprocessor', PolynomialFeatures(degree=deg, include_bias=True)),\n        ('estimator', LinearRegression())\n    ])\nHere’s an example of polynomial regression in action where \\(D = 3\\). Note that once we’ve defined or more complicated pipeline model, the fit and predict steps work just as before:\nfig, ax = plt.subplots(figsize=(8, 5))\n\nmodel = polynomial_regression(deg=3)\nmodel.fit(X, y)\ny_pred = model.predict(X)\n\nsns.scatterplot(data=df, x='x', y='y', ax = ax)\nsns.lineplot(x=df['x'], y=y_pred, color='black', ax =ax)\nt = ax.set_title(\n    f'Polynomial Regression (deg=3)\\nMSE: {mse(y, y_pred):.3f}'\n    )\nWith the incorporation of polynomial features, we are able to model the nonlinear trend in the data much more effectively.\nfig, ax = plt.subplots(2, 3, figsize=(6,4))\n\ndegs = [1, 3, 5, 10, 15, 20]\n\nfor i, deg in enumerate(degs):\n    model = polynomial_regression(deg=deg)\n\n    model.fit(X, y)\n    y_pred = model.predict(X)\n    axis = ax[i//3, i%3]\n    sns.scatterplot(data=df, x='x', y='y', ax = axis)\n    sns.lineplot(x=df['x'], y=y_pred, color='black', ax =axis)\n    axis.set_title(\n        f'Polynomial regression (deg={deg})\\nMSE: {mse(y, y_pred):.3f}'\n        )\n\nt = plt.tight_layout()\nAs \\(D\\) increases, we see that the model becomes more flexible and is able to fit the training data more closely. However, this flexibility comes at a cost: as \\(D\\) increases, the model begins to fit the noise in the data rather than the underlying trend, as reflected by the visual jags and wobbles.\nThis general phenomenon of fitting noise rather than signal is called overfitting. The trouble with overfitting is not that the model fits the training data poorly – in fact, overfitted models often have very low training error – but rather that they generalize poorly to new data. To see this, let’s generate some new test data from the same process:\ndf_val = sin_data(n_samples=30, noise_std=0.3)\nX_val = df_val[['x']]\ny_val = df_val['y']\n\n# create a model and fit it on the same features\nmodel = polynomial_regression(deg=25)\nmodel.fit(X, y)\ny_pred = model.predict(X)\n\nsns.scatterplot(data=df, x='x', y='y', label='Training Data', color = \"lightgrey\")\nsns.scatterplot(data=df_val, x='x', y='y', label='Validation Data')\nsns.lineplot(x=df['x'], y=y_pred, color='black', label='Model Prediction')\nt = plt.title(\n    f\"\"\"\n    Training MSE: {mse(y, model.predict(X)):.3f}\n    Validation MSE: {mse(y_val, model.predict(X_val)):.3f}\n    \"\"\"\n    )\nVisually, we can see that the model we’ve learned fits some of the noise in the training data, at the expense of a good fit on the test data.\nOne way to view the problem of overfitting is that in an overfit model, the training error is no longer a reliable guide to performance on new, unseen data. In the above example, we see that while the training MSE is very low, the validation MSE is much higher.\nWe can see where the training and validation MSE begin to diverge by systematically varying the polynomial degree and measuring both errors:\nmax_deg = 25\ndegs = np.arange(0, max_deg)\ntrain_mses      = []\nvalidation_mses = []\nfor deg in range(0, max_deg):\n    model = polynomial_regression(deg=deg)\n    model.fit(X, y)\n\n    y_train_pred = model.predict(X)\n    y_val_pred   = model.predict(X_val)\n\n    mse_train = mse(y, y_train_pred)\n    mse_val   = mse(y_val, y_val_pred)\n\n    train_mses.append(mse_train)\n    validation_mses.append(mse_val)\n\nsns.lineplot(x=degs, y=train_mses, marker='o', label='Train')\nsns.lineplot(x=degs, y=validation_mses, marker='o', label='Validation')\n\nbest_deg = np.argmin(validation_mses)\nsns.scatterplot(x=[best_deg], \n                y=[validation_mses[best_deg]], \n                color='black', \n                s=200)\n\n\nplt.xlabel('Polynomial Degree')\nplt.ylabel('Model score (mse)')\nplt.title('Overfitting in Polynomial Regression')\nl = plt.legend()\nIn this data set, the best model (as measured by performance on unseen data) occurs with polynomial degree 11.\n© Michael Linderman and Phil Chodrow, 2025",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/21-feature-engineering-and-overfitting.html#nonlinear-prediction-feature-maps",
    "href": "chapters/21-feature-engineering-and-overfitting.html#nonlinear-prediction-feature-maps",
    "title": "4  Features and Overfitting",
    "section": "",
    "text": "In this example with 1-dimensional input data, the choice of polynomial degree \\(D\\) is a hyperparameter which controls how flexible the model is. Larger choices of \\(D\\) lead to more flexible models which contain more parameters  How does our model performance depend on the choice of \\(D\\)? Let’s take a look at several choices:In this case, there are \\(D+1\\) parameters to fit.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/21-feature-engineering-and-overfitting.html#cross-validation-and-model-evaluation",
    "href": "chapters/21-feature-engineering-and-overfitting.html#cross-validation-and-model-evaluation",
    "title": "4  Features and Overfitting",
    "section": "Cross-Validation and Model Evaluation",
    "text": "Cross-Validation and Model Evaluation\nWhen we’re studying real data, we don’t usually have the opportunity to independently generate a separate validation set on which to assess our models. Instead, what we usually do is split the data we have. The typical workflow is:\n\nHold out a piece of the data which we won’t touch until the very end of our analysis, when we are ready to perform a final evaluation of our model. This is called the test set.\nUse the remaining data, called the training set, to fit models and perform model selection. This is where we will be tuning hyperparameters like polynomial degree.\nAlong the way, we’ll often want to assess how well our models are doing on unseen data. To do this, we can withold a portion of the training data to use as a validation set.\n\nTo practice this loop, let’s first generate a slightly larger data set from the same process.\n\ndf = sin_data(n_samples=200, noise_std=0.3)\nX = df[['x']]\ny = df['y']\n\nNext, we’ll split the data into training and test sets. We’ll hold out 20% of the data for testing, and use the remaining 80% for training.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nWe won’t touch either X_test or y_test until the very end of our analysis, when we are ready to perform a final evaluation of our model.\n\nSimulating Evaluation: Cross-Validation\nThe idea of cross-validation is that we can simulate the process of evaluating our model on unseen data by witholding parts of our training data to use as testing. In order to make an assessment, we can simulate the process of fitting the model and evaluating on “test” data by witholding parts of our training data to use as testing. We split the data into chunks and withold each chunk, using the other chunks to train the data. This is called cross-validation, and it is illustrated in this figure:\nVanderplas (2016) has more on cross-validation and overfitting. We’ll confront overfitting agian many times in this course.\n\n\n\nImage source: scikit-learn\n\n\nWe could do this with a for-loop, but the scikit-learn developers have implemented this for us. Here’s an example of cross-validation with 5 folds. This can take a little while, as there are actually 5 calls to model.fit() happening under the hood each time.\n\nfrom sklearn.model_selection import cross_val_score\nmodel = polynomial_regression(deg=3)\n\nscores = cross_val_score(\n    model, \n    X_train, \n    y_train, \n    cv=10, \n    scoring = \"neg_mean_squared_error\")\n\nprint(f\"MSEs for each fold: {np.array2string(-scores, precision=3)}\")\n\nprint(f\"The mean MSE from cross-validation is {-scores.mean():.3f}\")\n\nMSEs for each fold: [0.091 0.069 0.087 0.089 0.098 0.064 0.134 0.102 0.097 0.082]\nThe mean MSE from cross-validation is 0.091\n\n\nIf we wrap this in a loop, we can see how the cross-validated MSE changes as we vary the polynomial degree:\n\ncv_scores = []\ndegrees = np.arange(0, 10)\n\nfor deg in degrees:\n    deg_scores = cross_val_score(\n        polynomial_regression(deg=deg), \n        X_train, \n        y_train, \n        cv=10, \n        scoring = lambda model, X_, y_: mse(y_, model.predict(X_))\n    )\n    cv_scores.append(deg_scores.mean())\n    \nax = sns.lineplot(x=np.arange(0, len(cv_scores)), y=cv_scores, marker='o')\nax.set_xlabel('Polynomial Degree')\nax.set_ylabel('Cross-Validated MSE')\nt = ax.set_title('Cross-Validated MSE vs Polynomial Degree')\n\nbest_deg = degrees[np.argmin(cv_scores)]\n\nprint(f\"The best degree is {best_deg} with CV MSE of {min(cv_scores):.3f}\")\n\nThe best degree is 5 with CV MSE of 0.091\n\n\n\n\n\n\n\n\n\nHaving repeated model fitting and assessment over many different splits of the data, we can be more confident that our assessment of model performance reflects what we’d observe on unseen data, and that our choice of the degree hyperparameter is likely to perform well in predictive practice.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/21-feature-engineering-and-overfitting.html#unstructured-data-feature-extraction",
    "href": "chapters/21-feature-engineering-and-overfitting.html#unstructured-data-feature-extraction",
    "title": "4  Features and Overfitting",
    "section": "Unstructured Data: Feature Extraction",
    "text": "Unstructured Data: Feature Extraction\nIn the previous example, we began with data with a single feature \\(x\\), and then generated many new features \\(\\{\\phi_d(x)\\}\\) from it in order to model nonlinear patterns. This process is sometimes called feature engineering. Another way we sometimes work with features is called feature extraction, which most commonly appears when working with unstructured data like text or images.  Here’s an example of a data set containing Yelp reviews, along with a label giving the number of stars (1-5, represented in Python as 0 through 4) assigned by the reviewer:“Feature extraction” and “feature engineering” are often used interchangeably, and the difference between them can blur in pratice.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/refs/heads/main/data/yelp-reviews/reviews-subset.csv\"\ndf = pd.read_csv(url)\ndf.head()\n\n\n\n\n\n\n\n\nlabel\ntext\n\n\n\n\n0\n2\nGF and I tried today at lunch. As expected a n...\n\n\n1\n0\nChicken Tacos where good. Chips and guacamol...\n\n\n2\n2\nThings I liked about Graze:\\n1. The elderflowe...\n\n\n3\n3\nMichael Mina makes any restaurant sound intere...\n\n\n4\n2\nI was let down by yelp. As someone that trave...\n\n\n\n\n\n\n\nWe’d like to try predicting the label from the review text. To develop a model, let’s first perform a train-test split:\n\nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2)\n\nIn order to work with these data, we need to somehow represent the text through a set of numeric features. One way to approach this is to create a column for each of a set of words which we think might be meaningful. For example, words like “good” or “amazing” are likely associated to high-scoring reviews, while words like “awful” or “boring” may be associated with low-scoring reviews. For much more on how to use machine learning and other computational tools to study human language, take a course in natural language processing!\nHere’s an example in which we represent each of the reviews using 400 of the most common words in the training set. We can use a built-in tool from scikit-learn called CountVectorizer to do this for us: Writing this kind of tool by hand is a good exercise in Python programming and string manipulation!\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Create a CountVectorizer to convert text to term-document matrix\nvectorizer = CountVectorizer(max_features=400, stop_words='english')\nvectorizer.fit(X_train)\n\n# now we can construct the term-document matrix\nX_train_vec = pd.DataFrame(vectorizer.transform(X_train).toarray(), columns=vectorizer.get_feature_names_out())\n\nHere’s an excerpt with a few columns and rows from our training data:\n\nX_train_vec[[\"good\", \"bad\", \"best\", \"bland\", \"delicious\"]].head()\n\n\n\n\n\n\n\n\ngood\nbad\nbest\nbland\ndelicious\n\n\n\n\n0\n1\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n1\n0\n\n\n3\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nDoes this kind of information help us predict the review score? Let’s try fitting a linear regression model to see:\n\nLR = LinearRegression()\nmodel = LR.fit(X_train_vec, y_train)\nprint(\"Training MSE:\", mse(y_train, model.predict(X_train_vec)))\n\nTraining MSE: 1.0818071327467624\n\n\nAs a comparison, if we used a simple constant predictor equal to the mean of the training labels, the MSE would be np.float64(2.0662044374999997). The MSE we obtain from our linear model is much lower than this baseline MSE – looks promising!\nBefore we try evaluating our model, we should ask: was 400 really the right number of features to extract? Just as in the case of the polynomial regression model, we can (given enough computational power) approach this question using a systematic sweep over possibilities and cross-validation:\n\nscore_vec = []\n\npossible_num_features = 50*np.arange(1, 11)\n\nfor num_features in possible_num_features:\n    vectorizer = CountVectorizer(max_features=num_features, stop_words='english')\n    vectorizer.fit(X_train)\n\n    # construct the term-document matrix\n    X_train_vec = pd.DataFrame(vectorizer.transform(X_train).toarray(), columns=vectorizer.get_feature_names_out())\n    \n    LR = LinearRegression()\n\n    scores = cross_val_score(\n        LR, \n        X_train_vec, \n        y_train, \n        cv=10, \n        scoring='neg_mean_squared_error'\n    )\n    score_vec.append(-scores.mean())\n\nax = sns.lineplot(x=possible_num_features, y=score_vec, marker='o')\nax.set_xlabel('Number of Features Extracted')\ny = ax.set_ylabel('Cross-Validated MSE')\n\n\n\n\n\n\n\n\nIt looks like we might actually do a bit better with around 350 features. Let’s try that and finally evaluate on the test set:\n\nbest_num_features = possible_num_features[np.argmin(score_vec)]\nvectorizer = CountVectorizer(max_features=best_num_features, stop_words='english')\nvectorizer.fit(X_train)\n# construct the term-document matrix\nX_train_vec = pd.DataFrame(vectorizer.transform(X_train).toarray(), columns=vectorizer.get_feature_names_out())\nX_test_vec = pd.DataFrame(vectorizer.transform(X_test).toarray(), columns=vectorizer.get_feature_names_out())   \n\nLR = LinearRegression()\nLR.fit(X_train_vec, y_train)\n\nmse_test = mse(LR.predict(X_test_vec), y_test)\n\nprint(f\"The test MSE is {mse_test:.2f}\")\n\n\n\nprint(f\"The baseline constant model MSE is {mse(y_test, y_train.mean()):.2f}\")\n\nThe test MSE is 1.24\nThe baseline constant model MSE is 2.04\n\n\nThrough cross-validation, we’ve selected an informed guess for the model which will perform best on unseen future data.\n\nInterpreting Features\nWhat words are predictive of a good review? We can get some insight on this from the coefficients of the linear model, one of which is associated to each word. First we’ll gather the coefficients into a data frame:\n\nfeature_coefficients = pd.DataFrame({\n    'feature': X_train_vec.columns,\n    'coefficient': LR.coef_\n})\n\nThen we’ll look at the entries of the data frame with the highest and lowest coefficients:\n\n# good review indicators\nfeature_coefficients.sort_values(by='coefficient', ascending=False).head(5)\n\n\n\n\n\n\n\n\nfeature\ncoefficient\n\n\n\n\n96\nexcellent\n0.623757\n\n\n9\namazing\n0.517411\n\n\n18\nawesome\n0.481500\n\n\n24\nbest\n0.426713\n\n\n71\ndelicious\n0.393222\n\n\n\n\n\n\n\n\n# bad review indicators\nfeature_coefficients.sort_values(by='coefficient', ascending=True).head(5)\n\n\n\n\n\n\n\n\nfeature\ncoefficient\n\n\n\n\n302\nterrible\n-0.657938\n\n\n144\nhorrible\n-0.644740\n\n\n342\nworst\n-0.550828\n\n\n256\nrude\n-0.463192\n\n\n78\ndisappointed\n-0.375171\n\n\n\n\n\n\n\nIt appears our model has learned some reasonable associations about which words correspond to well-scored reviews, although much could be done to improve performance here.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/21-feature-engineering-and-overfitting.html#references",
    "href": "chapters/21-feature-engineering-and-overfitting.html#references",
    "title": "4  Features and Overfitting",
    "section": "References",
    "text": "References\n\n\n\n\nVanderplas, Jacob T. 2016. Python Data Science Handbook: Essential Tools for Working with Data. First edition. Sebastopol, CA: O’Reilly Media, Inc.",
    "crumbs": [
      "Examples",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Overfitting</span>"
    ]
  }
]