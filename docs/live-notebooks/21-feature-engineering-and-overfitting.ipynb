{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Features and Overfitting\n",
    "\n",
    "[Last time](20-introduction-prediction.qmd), we began our study of predictive modeling. We introduced the idea of fitting a model as an *optimization* problem, in which our aim is to minimize some *loss function* which measures the error between model predictions and the actual data. We then used this minimization idea to fit a linear regression model to a data set and measure how the fit model reduces the error when compared to a naive baseline model.\n",
    "\n",
    "However, in most of the systems we might wish to study, the relationship between the features (input) and target (output) is nonlinear. In this set of notes, we'll consider the problem of modeling nonlinear trends in data. Our approach to this problem will involve engineering many new features, which will in turn lead us to one of the fundamental risks in predictive modeling: overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nonlinear Prediction: Feature Maps\n",
    "\n",
    "To begin, let's consider some synthetic data. We'll write a simple function to generate data from a noisy sine wave:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def sin_data(n_samples=50, noise_std=0.3):\n",
    "    x = np.linspace(-3, 3, n_samples)\n",
    "    y = np.sin(x) + np.random.normal(0, noise_std, n_samples)\n",
    "    return pd.DataFrame({'x': x, 'y': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The resulting data set looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If we try to just fit a linear regression model, we'll be disappointed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, y_pred): \n",
    "    return np.mean((y - y_pred)**2)\n",
    "\n",
    "\n",
    "\n",
    "ax = sns.scatterplot(data=df, x='x', y='y')\n",
    "sns.lineplot(x=df['x'], y=y_pred, color='black', ax =ax)\n",
    "\n",
    "t = ax.set_title(\n",
    "    f'Linear Regression MSE on training data: {mse(y, y_pred):.3f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "One common paradigm for modeling nonlinear relationships is to *engineer new features* from the original data. This can be done by applying any nonlinear transformation to the original features. For example, we might define a set of *feature maps* $\\phi_d:\\mathbb{R} \\rightarrow \\mathbb{R}$ which accept an input $x$ and raise it to a power: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\phi_d(x) = x^d\\;.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can define as many of these as we like! Once we've computed these features, we can form a linear regression model using the expanded feature set: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat{y} = w_D\\phi_D(x) + w_{D-1}\\phi_{D-1}(x) + \\cdots + w_1 \\phi_1(x) + w_0 \\phi_0(x) = \\sum_{d=0}^D w_d \\phi_d(x)\\;.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that $\\phi_0(x) = x^0 = 1$, so here $w_0$ is playing the role of the bias term which we previously labeled $b$. This combination of linear regression with a polynomial feature map is often called *polynomial regression*. \n",
    "\n",
    "`scikit-learn` gives us a convenient way to implement polynomial regression with two components: the `PolynomialFeatures` preprocessor, which computes the polynomial features, and the `Pipeline` class, which allows us to chain together multiple steps (like preprocessing and estimation) into a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here's an example of polynomial regression in action where $D = 3$. Note that once we've defined or more complicated pipeline model, the `fit` and `predict` steps work just as before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "\n",
    "\n",
    "sns.scatterplot(data=df, x='x', y='y', ax = ax)\n",
    "sns.lineplot(x=df['x'], y=y_pred, color='black', ax =ax)\n",
    "t = ax.set_title(\n",
    "    f'Polynomial Regression (deg=3)\\nMSE: {mse(y, y_pred):.3f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "With the incorporation of polynomial features, we are able to model the nonlinear trend in the data much more effectively. \n",
    "\n",
    "In this example with 1-dimensional input data, the choice of polynomial degree $D$ is a *hyperparameter* which controls how flexible the model is. Larger choices of $D$ lead to more flexible models which contain more parameters [In this case, there are $D+1$ parameters to fit.]{.aside} How does our model performance depend on the choice of $D$? Let's take a look at several choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(6,4))\n",
    "\n",
    "degs = [1, 3, 5, 10, 15, 20]\n",
    "\n",
    "for i, deg in enumerate(degs):\n",
    "    model = polynomial_regression(deg=deg)\n",
    "\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    axis = ax[i//3, i%3]\n",
    "    sns.scatterplot(data=df, x='x', y='y', ax = axis)\n",
    "    sns.lineplot(x=df['x'], y=y_pred, color='black', ax =axis)\n",
    "    axis.set_title(\n",
    "        f'Polynomial regression (deg={deg})\\nMSE: {mse(y, y_pred):.3f}'\n",
    "        )\n",
    "\n",
    "t = plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As $D$ increases, we see that the model becomes more flexible and is able to fit the training data more closely. However, this flexibility comes at a cost: as $D$ increases, the model begins to fit the *noise* in the data rather than the underlying trend, as reflected by the visual jags and wobbles. \n",
    "\n",
    "This general phenomenon of fitting noise rather than signal is called *overfitting*. The trouble with overfitting is not that the model fits the training data poorly -- in fact, overfitted models often have very low training error -- but rather that they generalize poorly to new data. To see this, let's generate some new test data from the same process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = sin_data(n_samples=30, noise_std=0.3)\n",
    "X_val = df_val[['x']]\n",
    "y_val = df_val['y']\n",
    "\n",
    "# create a model and fit it on the same features\n",
    "model = polynomial_regression(deg=25)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "sns.scatterplot(data=df, x='x', y='y', label='Training Data', color = \"lightgrey\")\n",
    "sns.scatterplot(data=df_val, x='x', y='y', label='Validation Data')\n",
    "sns.lineplot(x=df['x'], y=y_pred, color='black', label='Model Prediction')\n",
    "t = plt.title(\n",
    "    f\"\"\"\n",
    "    Training MSE: {mse(y, model.predict(X)):.3f}\n",
    "    Validation MSE: {mse(y_val, model.predict(X_val)):.3f}\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visually, we can see that the model we've learned fits some of the noise in the training data, at the expense of a good fit on the test data. \n",
    "\n",
    "One way to view the problem of overfitting is that in an overfit model, *the training error is no longer a reliable guide to performance on new, unseen data*. In the above example, we see that while the training MSE is very low, the validation MSE is much higher.\n",
    "\n",
    "We can see where the training and validation MSE begin to diverge by systematically varying the polynomial degree and measuring both errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_deg = 25\n",
    "degs = np.arange(0, max_deg)\n",
    "train_mses      = []\n",
    "validation_mses = []\n",
    "for deg in range(0, max_deg):\n",
    "    model = polynomial_regression(deg=deg)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    y_train_pred = model.predict(X)\n",
    "    y_val_pred   = model.predict(X_val)\n",
    "\n",
    "    mse_train = mse(y, y_train_pred)\n",
    "    mse_val   = mse(y_val, y_val_pred)\n",
    "\n",
    "    train_mses.append(mse_train)\n",
    "    validation_mses.append(mse_val)\n",
    "\n",
    "sns.lineplot(x=degs, y=train_mses, marker='o', label='Train')\n",
    "sns.lineplot(x=degs, y=validation_mses, marker='o', label='Validation')\n",
    "\n",
    "best_deg = np.argmin(validation_mses)\n",
    "sns.scatterplot(x=[best_deg], \n",
    "                y=[validation_mses[best_deg]], \n",
    "                color='black', \n",
    "                s=200)\n",
    "\n",
    "\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Model score (mse)')\n",
    "plt.title('Overfitting in Polynomial Regression')\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this data set, the best model (as measured by performance on unseen data) occurs with polynomial degree `{python} int(best_deg)`. \n",
    "\n",
    "## Cross-Validation and Model Evaluation\n",
    "\n",
    "When we're studying real data, we don't usually have the opportunity to independently generate a separate validation set on which to assess our models. Instead, what we usually do is split the data we have. The typical workflow is: \n",
    "\n",
    "1. Hold out a piece of the data which we won't touch until the very end of our analysis, when we are ready to perform a final evaluation of our model. This is called the *test set*.\n",
    "2. Use the remaining data, called the *training set*, to fit models and perform model selection. This is where we will be tuning hyperparameters like polynomial degree.\n",
    "3. Along the way, we'll often want to assess how well our models are doing on unseen data. To do this, we can withold a portion of the training data to use as a *validation set*.\n",
    "\n",
    "To practice this loop, let's first generate a slightly larger data set from the same process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sin_data(n_samples=200, noise_std=0.3)\n",
    "X = df[['x']]\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next, we'll split the data into training and test sets. We'll hold out 20% of the data for testing, and use the remaining 80% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We won't touch either `X_test` or `y_test` until the very end of our analysis, when we are ready to perform a final evaluation of our model.\n",
    "\n",
    "### Simulating Evaluation: Cross-Validation\n",
    "\n",
    "The idea of cross-validation is that we can simulate the process of evaluating our model on unseen data by witholding parts of our training data to use as testing. In order to make an assessment, we can simulate the process of fitting the model and evaluating on \"test\" data by witholding parts of our *training* data to use as testing. We split the data into chunks and withold each chunk, using the other chunks to train the data. This is called *cross-validation*, and it is illustrated in this figure: \n",
    "\n",
    "[@vanderplasPythonDataScience2016 has [more on cross-validation](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html) and overfitting. We'll confront overfitting agian many times in this course.]{.aside}\n",
    "\n",
    "![Image source: [scikit-learn](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png) ](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "We could do this with a  for-loop, but the `scikit-learn` developers have implemented this for us. Here's an example of cross-validation with 5 folds. This can take a little while, as there are actually 5 calls to `model.fit()` happening under the hood each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"MSEs for each fold: {np.array2string(-scores, precision=3)}\")\n",
    "\n",
    "print(f\"The mean MSE from cross-validation is {-scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If we wrap this in a loop, we can see how the cross-validated MSE changes as we vary the polynomial degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "degrees = np.arange(0, 10)\n",
    "\n",
    "for deg in degrees:\n",
    "    deg_scores = cross_val_score(\n",
    "        polynomial_regression(deg=deg), \n",
    "        X_train, \n",
    "        y_train, \n",
    "        cv=10, \n",
    "        scoring = lambda model, X_, y_: mse(y_, model.predict(X_))\n",
    "    )\n",
    "    cv_scores.append(deg_scores.mean())\n",
    "    \n",
    "ax = sns.lineplot(x=np.arange(0, len(cv_scores)), y=cv_scores, marker='o')\n",
    "ax.set_xlabel('Polynomial Degree')\n",
    "ax.set_ylabel('Cross-Validated MSE')\n",
    "t = ax.set_title('Cross-Validated MSE vs Polynomial Degree')\n",
    "\n",
    "best_deg = degrees[np.argmin(cv_scores)]\n",
    "\n",
    "print(f\"The best degree is {best_deg} with CV MSE of {min(cv_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Having repeated model fitting and assessment over many different splits of the data, we can be more confident that our assessment of model performance reflects what we'd observe on unseen data, and that our choice of the degree hyperparameter is likely to perform well in predictive practice. \n",
    "\n",
    "## Unstructured Data: Feature Extraction\n",
    "\n",
    "In the previous example, we began with data with a single feature $x$, and then generated many new features $\\{\\phi_d(x)\\}$ from it in order to model nonlinear patterns. This process is sometimes called *feature engineering*. Another way we sometimes work with features is called *feature extraction*, which most commonly appears when working with unstructured data like text or images. [\"*Feature extraction*\" and \"*feature engineering*\" are often used interchangeably, and the difference between them can blur in pratice.]{.aside} Here's an example of a data set containing Yelp reviews, along with a label giving the number of stars (1-5, represented in Python as `0` through `4`) assigned by the reviewer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/refs/heads/main/data/yelp-reviews/reviews-subset.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We'd like to try predicting the label from the review text. To develop a model, let's first perform a train-test split: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In order to work with these data, we need to somehow represent the text through a set of numeric features. One way to approach this is to create a column for each of a set of *words* which we think might be meaningful. For example, words like \"good\" or \"amazing\" are likely associated to high-scoring reviews, while words like \"awful\" or \"boring\" may be associated with low-scoring reviews. [For much more on how to use machine learning and other computational tools to study human language, take a course in natural language processing!]{.aside}\n",
    "\n",
    "Here's an example in which we represent each of the reviews using 400 of the most common words in the training set. We can use a built-in tool from `scikit-learn` called `CountVectorizer` to do this for us: [Writing this kind of tool by hand is a good exercise in Python programming and string manipulation!]{.aside}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here's an excerpt with a few columns and rows from our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec[[\"good\", \"bad\", \"best\", \"bland\", \"delicious\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Does this kind of information help us predict the review score? Let's try fitting a linear regression model to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Training MSE:\", mse(y_train, model.predict(X_train_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As a comparison, if we used a simple constant predictor equal to the mean of the training labels, the MSE would be `{python} mse(y_train, y_train.mean())`. The MSE we obtain from our linear model is much lower than this baseline MSE -- looks promising! \n",
    "\n",
    "Before we try evaluating our model, we should ask: was 400 really the right number of features to extract? Just as in the case of the polynomial regression model, we can (given enough computational power) approach this question using a systematic sweep over possibilities and cross-validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_vec = []\n",
    "\n",
    "possible_num_features = 50*np.arange(1, 11)\n",
    "\n",
    "for num_features in possible_num_features:\n",
    "    vectorizer = CountVectorizer(max_features=num_features, stop_words='english')\n",
    "    vectorizer.fit(X_train)\n",
    "\n",
    "    # construct the term-document matrix\n",
    "    X_train_vec = pd.DataFrame(vectorizer.transform(X_train).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    LR = LinearRegression()\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        LR, \n",
    "        X_train_vec, \n",
    "        y_train, \n",
    "        cv=10, \n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    score_vec.append(-scores.mean())\n",
    "\n",
    "ax = sns.lineplot(x=possible_num_features, y=score_vec, marker='o')\n",
    "ax.set_xlabel('Number of Features Extracted')\n",
    "y = ax.set_ylabel('Cross-Validated MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It looks like we might actually do a bit better with around 350 features. Let's try that and finally evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_features = possible_num_features[np.argmin(score_vec)]\n",
    "vectorizer = CountVectorizer(max_features=best_num_features, stop_words='english')\n",
    "vectorizer.fit(X_train)\n",
    "# construct the term-document matrix\n",
    "X_train_vec = pd.DataFrame(vectorizer.transform(X_train).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "X_test_vec = pd.DataFrame(vectorizer.transform(X_test).toarray(), columns=vectorizer.get_feature_names_out())   \n",
    "\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train_vec, y_train)\n",
    "\n",
    "mse_test = mse(LR.predict(X_test_vec), y_test)\n",
    "\n",
    "print(f\"The test MSE is {mse_test:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"The baseline constant model MSE is {mse(y_test, y_train.mean()):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Through cross-validation, we've selected an informed guess for the model which will perform best on unseen future data. \n",
    "\n",
    "### Interpreting Features\n",
    "\n",
    "What words are predictive of a good review? We can get some insight on this from the coefficients of the linear model, one of which is associated to each word. First we'll gather  the coefficients into a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coefficients = pd.DataFrame({\n",
    "    'feature': X_train_vec.columns,\n",
    "    'coefficient': LR.coef_\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Then we'll look at the entries of the data frame with the highest and lowest coefficients: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good review indicators\n",
    "feature_coefficients.sort_values(by='coefficient', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad review indicators\n",
    "feature_coefficients.sort_values(by='coefficient', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It appears our model has learned some reasonable associations about which words correspond to well-scored reviews, although much could be done to improve performance here. \n",
    "\n",
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/philchodrow/My Drive (pchodrow@middlebury.edu)/teaching/ml-notes/env/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
